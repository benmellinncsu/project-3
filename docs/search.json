[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "I will be working with the diabetes data set from kaggle.\nMy goal will be to make predictions about if an individual has diabetes/prediabetes based on predictors that are related to that individual’s lifestyle.\nThis document will be exploring the data.\nI will be ignoring medical predictors, like blood pressure or cholesterol level.\nHere are the final list of predictors I will be using:\n\nIf the person has smoked 100 cigarettes in their lifetime\nIf they had physical activity in the last 30 days\nIf they consume 1 or more fruit per day\nIf they consume 1 or more vegetable in the past day\nIf they are heavy alcohol drinkers\nAge\nEducation\nIncome\n\n\n\n\n\n\n\nlibrary(readr)\n\ndata &lt;- read_csv(\"../diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\n\n\n\n\n#dimensions\ndim(data)\n\n[1] 253680     22\n\n# na data\ncolSums(is.na(data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# look at unique values\nlapply(data, unique)\n\n$Diabetes_binary\n[1] 0 1\n\n$HighBP\n[1] 1 0\n\n$HighChol\n[1] 1 0\n\n$CholCheck\n[1] 1 0\n\n$BMI\n [1] 40 25 28 27 24 30 34 26 33 21 23 22 38 32 37 31 29 20 35 45 39 19 47 18 36\n[26] 43 55 49 42 17 16 41 44 50 59 48 52 46 54 57 53 14 15 51 58 63 61 56 74 62\n[51] 64 66 73 85 60 67 65 70 82 79 92 68 72 88 96 13 81 71 75 12 77 69 76 87 89\n[76] 84 95 98 91 86 83 80 90 78\n\n$Smoker\n[1] 1 0\n\n$Stroke\n[1] 0 1\n\n$HeartDiseaseorAttack\n[1] 0 1\n\n$PhysActivity\n[1] 0 1\n\n$Fruits\n[1] 0 1\n\n$Veggies\n[1] 1 0\n\n$HvyAlcoholConsump\n[1] 0 1\n\n$AnyHealthcare\n[1] 1 0\n\n$NoDocbcCost\n[1] 0 1\n\n$GenHlth\n[1] 5 3 2 4 1\n\n$MentHlth\n [1] 18  0 30  3  5 15 10  6 20  2 25  1  4  7  8 21 14 26 29 16 28 11 12 24 17\n[26] 13 27 19 22  9 23\n\n$PhysHlth\n [1] 15  0 30  2 14 28  7 20  3 10  1  5 17  4 19  6 12 25 27 21 22  8 29 24  9\n[26] 16 18 23 13 26 11\n\n$DiffWalk\n[1] 1 0\n\n$Sex\n[1] 0 1\n\n$Age\n [1]  9  7 11 10  8 13  4  6  2 12  5  1  3\n\n$Education\n[1] 4 6 3 5 2 1\n\n$Income\n[1] 3 1 8 6 4 7 2 5\n\n\nNo missing data.\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_subset &lt;- data |&gt;\n  select(Diabetes_binary, Smoker, PhysActivity, Fruits, Veggies,\n         HvyAlcoholConsump, Sex, Education, Income)\n\n# Check the result\nhead(data_subset)\n\n# A tibble: 6 × 9\n  Diabetes_binary Smoker PhysActivity Fruits Veggies HvyAlcoholConsump   Sex\n            &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1               0      1            0      0       1                 0     0\n2               0      1            1      0       0                 0     0\n3               0      0            0      1       0                 0     0\n4               0      0            1      1       1                 0     0\n5               0      0            1      1       1                 0     0\n6               0      1            1      1       1                 0     1\n# ℹ 2 more variables: Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\n\n\n\n\ndata_subset &lt;- data_subset |&gt;\n  mutate(\n    Diabetes_binary   = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Smoker            = factor(Smoker, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity      = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Fruits            = factor(Fruits, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Veggies           = factor(Veggies, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Sex               = factor(Sex, levels = c(0,1), labels = c(\"Male\", \"Female\")),\n    Education         = as.factor(Education),\n    Income            = as.factor(Income)\n  )\n\n# Check structure\nstr(data_subset)\n\ntibble [253,680 × 9] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ Smoker           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ PhysActivity     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies          : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Sex              : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Education        : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income           : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\nsummary(data_subset)\n\n Diabetes_binary Smoker       PhysActivity Fruits       Veggies     \n No :218334      No :141257   No : 61760   No : 92782   No : 47839  \n Yes: 35346      Yes:112423   Yes:191920   Yes:160898   Yes:205841  \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n HvyAlcoholConsump     Sex         Education      Income     \n No :239424        Male  :141974   1:   174   8      :90385  \n Yes: 14256        Female:111706   2:  4043   7      :43219  \n                                   3:  9478   6      :36470  \n                                   4: 62750   5      :25883  \n                                   5: 69910   4      :20135  \n                                   6:107325   3      :15994  \n                                              (Other):21594  \n\n\n\n\n\nI will create contingency tables between the variables and diabetes_binary.\n\nfor (col in names(data_subset)) {\n  if (col != \"Diabetes_binary\") {\n    cat(\"\\n Contingency table:\", col, \"vs Diabetes_binary\")\n    print(table(data_subset[[col]], data_subset$Diabetes_binary))\n  }\n}\n\n\n Contingency table: Smoker vs Diabetes_binary     \n          No    Yes\n  No  124228  17029\n  Yes  94106  18317\n\n Contingency table: PhysActivity vs Diabetes_binary     \n          No    Yes\n  No   48701  13059\n  Yes 169633  22287\n\n Contingency table: Fruits vs Diabetes_binary     \n          No    Yes\n  No   78129  14653\n  Yes 140205  20693\n\n Contingency table: Veggies vs Diabetes_binary     \n          No    Yes\n  No   39229   8610\n  Yes 179105  26736\n\n Contingency table: HvyAlcoholConsump vs Diabetes_binary     \n          No    Yes\n  No  204910  34514\n  Yes  13424    832\n\n Contingency table: Sex vs Diabetes_binary        \n             No    Yes\n  Male   123563  18411\n  Female  94771  16935\n\n Contingency table: Education vs Diabetes_binary   \n       No   Yes\n  1   127    47\n  2  2860  1183\n  3  7182  2296\n  4 51684 11066\n  5 59556 10354\n  6 96925 10400\n\n Contingency table: Income vs Diabetes_binary   \n       No   Yes\n  1  7428  2383\n  2  8697  3086\n  3 12426  3568\n  4 16081  4054\n  5 21379  4504\n  6 31179  5291\n  7 37954  5265\n  8 83190  7195\n\n\n\n## Look at percentages\n\nfor (col in names(data_subset)) {\n  if (col != \"Diabetes_binary\") {\n    cat(\"\\n--- Contingency table (percentages):\", col, \"vs Diabetes_binary ---\\n\")\n    \n   \n    tbl &lt;- table(data_subset[[col]], data_subset$Diabetes_binary)\n    \n    \n    tbl_pct &lt;- prop.table(tbl, margin = 1) * 100\n    \n    # Round\n    print(round(tbl_pct, 1))\n  }\n}\n\n\n--- Contingency table (percentages): Smoker vs Diabetes_binary ---\n     \n        No  Yes\n  No  87.9 12.1\n  Yes 83.7 16.3\n\n--- Contingency table (percentages): PhysActivity vs Diabetes_binary ---\n     \n        No  Yes\n  No  78.9 21.1\n  Yes 88.4 11.6\n\n--- Contingency table (percentages): Fruits vs Diabetes_binary ---\n     \n        No  Yes\n  No  84.2 15.8\n  Yes 87.1 12.9\n\n--- Contingency table (percentages): Veggies vs Diabetes_binary ---\n     \n      No Yes\n  No  82  18\n  Yes 87  13\n\n--- Contingency table (percentages): HvyAlcoholConsump vs Diabetes_binary ---\n     \n        No  Yes\n  No  85.6 14.4\n  Yes 94.2  5.8\n\n--- Contingency table (percentages): Sex vs Diabetes_binary ---\n        \n           No  Yes\n  Male   87.0 13.0\n  Female 84.8 15.2\n\n--- Contingency table (percentages): Education vs Diabetes_binary ---\n   \n      No  Yes\n  1 73.0 27.0\n  2 70.7 29.3\n  3 75.8 24.2\n  4 82.4 17.6\n  5 85.2 14.8\n  6 90.3  9.7\n\n--- Contingency table (percentages): Income vs Diabetes_binary ---\n   \n      No  Yes\n  1 75.7 24.3\n  2 73.8 26.2\n  3 77.7 22.3\n  4 79.9 20.1\n  5 82.6 17.4\n  6 85.5 14.5\n  7 87.8 12.2\n  8 92.0  8.0\n\n\nBased on the percentages, I learned that there are relationships between all variables and whether or not someone has diabetes. in each case, the difference is relatively small, but I’m hoping that the cumulative effect of them will be enough to make predictions.\nNotably, the greater your income and education level, the less likely you are to have diabetes.\nFurthermore, classic lifestyle advice, like eating fruits, veggies, and getting physical activity is associated with a decreased likelihood of diabetes.\nNext, take a look at my modeling page to see how I model the data."
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "EDA",
    "section": "",
    "text": "I will be working with the diabetes data set from kaggle.\nMy goal will be to make predictions about if an individual has diabetes/prediabetes based on predictors that are related to that individual’s lifestyle.\nThis document will be exploring the data.\nI will be ignoring medical predictors, like blood pressure or cholesterol level.\nHere are the final list of predictors I will be using:\n\nIf the person has smoked 100 cigarettes in their lifetime\nIf they had physical activity in the last 30 days\nIf they consume 1 or more fruit per day\nIf they consume 1 or more vegetable in the past day\nIf they are heavy alcohol drinkers\nAge\nEducation\nIncome\n\n\n\n\n\n\n\nlibrary(readr)\n\ndata &lt;- read_csv(\"../diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\n\n\n\n\n#dimensions\ndim(data)\n\n[1] 253680     22\n\n# na data\ncolSums(is.na(data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n# look at unique values\nlapply(data, unique)\n\n$Diabetes_binary\n[1] 0 1\n\n$HighBP\n[1] 1 0\n\n$HighChol\n[1] 1 0\n\n$CholCheck\n[1] 1 0\n\n$BMI\n [1] 40 25 28 27 24 30 34 26 33 21 23 22 38 32 37 31 29 20 35 45 39 19 47 18 36\n[26] 43 55 49 42 17 16 41 44 50 59 48 52 46 54 57 53 14 15 51 58 63 61 56 74 62\n[51] 64 66 73 85 60 67 65 70 82 79 92 68 72 88 96 13 81 71 75 12 77 69 76 87 89\n[76] 84 95 98 91 86 83 80 90 78\n\n$Smoker\n[1] 1 0\n\n$Stroke\n[1] 0 1\n\n$HeartDiseaseorAttack\n[1] 0 1\n\n$PhysActivity\n[1] 0 1\n\n$Fruits\n[1] 0 1\n\n$Veggies\n[1] 1 0\n\n$HvyAlcoholConsump\n[1] 0 1\n\n$AnyHealthcare\n[1] 1 0\n\n$NoDocbcCost\n[1] 0 1\n\n$GenHlth\n[1] 5 3 2 4 1\n\n$MentHlth\n [1] 18  0 30  3  5 15 10  6 20  2 25  1  4  7  8 21 14 26 29 16 28 11 12 24 17\n[26] 13 27 19 22  9 23\n\n$PhysHlth\n [1] 15  0 30  2 14 28  7 20  3 10  1  5 17  4 19  6 12 25 27 21 22  8 29 24  9\n[26] 16 18 23 13 26 11\n\n$DiffWalk\n[1] 1 0\n\n$Sex\n[1] 0 1\n\n$Age\n [1]  9  7 11 10  8 13  4  6  2 12  5  1  3\n\n$Education\n[1] 4 6 3 5 2 1\n\n$Income\n[1] 3 1 8 6 4 7 2 5\n\n\nNo missing data.\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata_subset &lt;- data |&gt;\n  select(Diabetes_binary, Smoker, PhysActivity, Fruits, Veggies,\n         HvyAlcoholConsump, Sex, Education, Income)\n\n# Check the result\nhead(data_subset)\n\n# A tibble: 6 × 9\n  Diabetes_binary Smoker PhysActivity Fruits Veggies HvyAlcoholConsump   Sex\n            &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1               0      1            0      0       1                 0     0\n2               0      1            1      0       0                 0     0\n3               0      0            0      1       0                 0     0\n4               0      0            1      1       1                 0     0\n5               0      0            1      1       1                 0     0\n6               0      1            1      1       1                 0     1\n# ℹ 2 more variables: Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\n\n\n\n\ndata_subset &lt;- data_subset |&gt;\n  mutate(\n    Diabetes_binary   = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Smoker            = factor(Smoker, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity      = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Fruits            = factor(Fruits, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Veggies           = factor(Veggies, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Sex               = factor(Sex, levels = c(0,1), labels = c(\"Male\", \"Female\")),\n    Education         = as.factor(Education),\n    Income            = as.factor(Income)\n  )\n\n# Check structure\nstr(data_subset)\n\ntibble [253,680 × 9] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ Smoker           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ PhysActivity     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits           : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies          : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Sex              : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Education        : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income           : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\nsummary(data_subset)\n\n Diabetes_binary Smoker       PhysActivity Fruits       Veggies     \n No :218334      No :141257   No : 61760   No : 92782   No : 47839  \n Yes: 35346      Yes:112423   Yes:191920   Yes:160898   Yes:205841  \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n HvyAlcoholConsump     Sex         Education      Income     \n No :239424        Male  :141974   1:   174   8      :90385  \n Yes: 14256        Female:111706   2:  4043   7      :43219  \n                                   3:  9478   6      :36470  \n                                   4: 62750   5      :25883  \n                                   5: 69910   4      :20135  \n                                   6:107325   3      :15994  \n                                              (Other):21594  \n\n\n\n\n\nI will create contingency tables between the variables and diabetes_binary.\n\nfor (col in names(data_subset)) {\n  if (col != \"Diabetes_binary\") {\n    cat(\"\\n Contingency table:\", col, \"vs Diabetes_binary\")\n    print(table(data_subset[[col]], data_subset$Diabetes_binary))\n  }\n}\n\n\n Contingency table: Smoker vs Diabetes_binary     \n          No    Yes\n  No  124228  17029\n  Yes  94106  18317\n\n Contingency table: PhysActivity vs Diabetes_binary     \n          No    Yes\n  No   48701  13059\n  Yes 169633  22287\n\n Contingency table: Fruits vs Diabetes_binary     \n          No    Yes\n  No   78129  14653\n  Yes 140205  20693\n\n Contingency table: Veggies vs Diabetes_binary     \n          No    Yes\n  No   39229   8610\n  Yes 179105  26736\n\n Contingency table: HvyAlcoholConsump vs Diabetes_binary     \n          No    Yes\n  No  204910  34514\n  Yes  13424    832\n\n Contingency table: Sex vs Diabetes_binary        \n             No    Yes\n  Male   123563  18411\n  Female  94771  16935\n\n Contingency table: Education vs Diabetes_binary   \n       No   Yes\n  1   127    47\n  2  2860  1183\n  3  7182  2296\n  4 51684 11066\n  5 59556 10354\n  6 96925 10400\n\n Contingency table: Income vs Diabetes_binary   \n       No   Yes\n  1  7428  2383\n  2  8697  3086\n  3 12426  3568\n  4 16081  4054\n  5 21379  4504\n  6 31179  5291\n  7 37954  5265\n  8 83190  7195\n\n\n\n## Look at percentages\n\nfor (col in names(data_subset)) {\n  if (col != \"Diabetes_binary\") {\n    cat(\"\\n--- Contingency table (percentages):\", col, \"vs Diabetes_binary ---\\n\")\n    \n   \n    tbl &lt;- table(data_subset[[col]], data_subset$Diabetes_binary)\n    \n    \n    tbl_pct &lt;- prop.table(tbl, margin = 1) * 100\n    \n    # Round\n    print(round(tbl_pct, 1))\n  }\n}\n\n\n--- Contingency table (percentages): Smoker vs Diabetes_binary ---\n     \n        No  Yes\n  No  87.9 12.1\n  Yes 83.7 16.3\n\n--- Contingency table (percentages): PhysActivity vs Diabetes_binary ---\n     \n        No  Yes\n  No  78.9 21.1\n  Yes 88.4 11.6\n\n--- Contingency table (percentages): Fruits vs Diabetes_binary ---\n     \n        No  Yes\n  No  84.2 15.8\n  Yes 87.1 12.9\n\n--- Contingency table (percentages): Veggies vs Diabetes_binary ---\n     \n      No Yes\n  No  82  18\n  Yes 87  13\n\n--- Contingency table (percentages): HvyAlcoholConsump vs Diabetes_binary ---\n     \n        No  Yes\n  No  85.6 14.4\n  Yes 94.2  5.8\n\n--- Contingency table (percentages): Sex vs Diabetes_binary ---\n        \n           No  Yes\n  Male   87.0 13.0\n  Female 84.8 15.2\n\n--- Contingency table (percentages): Education vs Diabetes_binary ---\n   \n      No  Yes\n  1 73.0 27.0\n  2 70.7 29.3\n  3 75.8 24.2\n  4 82.4 17.6\n  5 85.2 14.8\n  6 90.3  9.7\n\n--- Contingency table (percentages): Income vs Diabetes_binary ---\n   \n      No  Yes\n  1 75.7 24.3\n  2 73.8 26.2\n  3 77.7 22.3\n  4 79.9 20.1\n  5 82.6 17.4\n  6 85.5 14.5\n  7 87.8 12.2\n  8 92.0  8.0\n\n\nBased on the percentages, I learned that there are relationships between all variables and whether or not someone has diabetes. in each case, the difference is relatively small, but I’m hoping that the cumulative effect of them will be enough to make predictions.\nNotably, the greater your income and education level, the less likely you are to have diabetes.\nFurthermore, classic lifestyle advice, like eating fruits, veggies, and getting physical activity is associated with a decreased likelihood of diabetes.\nNext, take a look at my modeling page to see how I model the data."
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "I will be working with the diabetes data set from kaggle.\nMy goal will be to make predictions about if an individual has diabetes/prediabetes based on predictors that are related to that individual’s lifestyle.\nThis document will be modeling the data with a classification tree and random forest.\nI will be ignoring medical predictors, like blood pressure or cholesterol level.\nHere are the final list of predictors I will be using:\n\nIf the person has smoked 100 cigarettes in their lifetime\nIf they had physical activity in the last 30 days\nIf they consume 1 or more fruit per day\nIf they consume 1 or more vegetable in the past day\nIf they are heavy alcohol drinkers\nAge\nEducation\nIncome\n\n\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#load the data\ndata &lt;- read_csv(\"../diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# subset only the relevant predictors\ndata_subset &lt;- data |&gt;\n  select(Diabetes_binary, Smoker, PhysActivity, Fruits, Veggies,\n         HvyAlcoholConsump, Sex, Education, Income)\n\n#convert all features to factors\ndata_subset &lt;- data_subset |&gt;\n  mutate(\n    Diabetes_binary   = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Smoker            = factor(Smoker, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity      = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Fruits            = factor(Fruits, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Veggies           = factor(Veggies, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Sex               = factor(Sex, levels = c(0,1), labels = c(\"Male\", \"Female\")),\n    Education         = as.factor(Education),\n    Income            = as.factor(Income)\n  )\n\n\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nset.seed(222)\n# Put 70% of the data into the training set \ndata_split &lt;- initial_split(data_subset, prop = .7)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\n#check\ndim(test_data)\n\n[1] 76104     9\n\ndim(train_data)\n\n[1] 177576      9\n\n\n\n\n\n\nset.seed(222)\nfolds &lt;- vfold_cv(train_data, v = 5)\n\n\n\n\nThe classification tree is a greedy algorithm that predicts a categorical label, in this case that label is whether someone has diabetes/prediabetes or not.\n\nThe algorithm iteratively splits the data based on the “best” feature. The “best” feature is the feature that maximizes the “purity” of the resulting groups, or that keeps the resulting groups as homogeneous as possible.\nTo split the data, the algorithm divides the data into subsets based on values of the predictor variable.\nThis process is repeated until a desired tree size is reached.\nTo classify a new instance, simply start at the trees root, and follow the nodes down until a leaf is reached.\n\n\nset.seed(222)\n\n#specify the model\n\ntune_spec &lt;- decision_tree(\n  cost_complexity = tune(),  \n  tree_depth = NULL          \n) |&gt;\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_grid &lt;- tibble(\n  cost_complexity = c(0, 0.001, 0.01, 0.05, 0.1)\n)\n\nset.seed(222)\ntree_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(Diabetes_binary ~ .)\n\ntree_res &lt;- \n  tree_wf |&gt; \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    metrics = metric_set(mn_log_loss)\n    )\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics         .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [142060/35516]&gt; Fold1 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142061/35515]&gt; Fold2 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142061/35515]&gt; Fold3 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35515]&gt; Fold4 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35515]&gt; Fold5 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n\n\n## View results\n\n\nlogloss_results &lt;- tree_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") \n\nprint(logloss_results)\n\n# A tibble: 5 × 7\n  cost_complexity .metric     .estimator  mean     n std_err .config        \n            &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1           0     mn_log_loss binary     0.388     5 0.00227 pre0_mod1_post0\n2           0.001 mn_log_loss binary     0.404     5 0.00116 pre0_mod2_post0\n3           0.01  mn_log_loss binary     0.404     5 0.00116 pre0_mod3_post0\n4           0.05  mn_log_loss binary     0.404     5 0.00116 pre0_mod4_post0\n5           0.1   mn_log_loss binary     0.404     5 0.00116 pre0_mod5_post0\n\n\nSeems like the tree with 0 as a value of cost complexity did the best. We want to minimize log loss.\n\n## Select the best tree\nbest_tree &lt;- tree_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nbest_tree\n\n# A tibble: 1 × 2\n  cost_complexity .config        \n            &lt;dbl&gt; &lt;chr&gt;          \n1               0 pre0_mod1_post0\n\n##Finalize workflow\n\nfinal_tree_wf &lt;- \n  tree_wf %&gt;% \n  finalize_workflow(best_tree)\n\nfinal_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nDiabetes_binary ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 0\n\nComputational engine: rpart \n\n\n\n\n\nRandom forests are similar to decision trees, except they rely on many trained decision trees. Each individual decision tree is built on a random subset of the predictors. Predictions are made based on voting from the subset of decision trees, which can make it less prone to overfitting and more stable compared to a basic, simple tree.\n\nlibrary(ranger)\n\nset.seed(222)\n\n# Specify the model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),      \n  trees = 50,        \n  min_n = 5    ## make the fitting time faster       \n) |&gt;\n  set_engine(\"ranger\", num.threads = parallel::detectCores()) |&gt;\n  set_mode(\"classification\")\n\n\nrf_grid &lt;- tibble(\n  mtry = c(2, 4, 6, 8) \n)\n\nset.seed(222)\nrf_wf &lt;- workflow() |&gt;\n  add_model(rf_spec) |&gt;\n  add_formula(Diabetes_binary ~ .)\n\nrf_res &lt;-\n  rf_wf |&gt;\n  tune_grid(\n    resamples = folds,\n    grid = rf_grid,\n    metrics = metric_set(mn_log_loss)\n  )\n\nrf_res\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics         .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [142060/35516]&gt; Fold1 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142061/35515]&gt; Fold2 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142061/35515]&gt; Fold3 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35515]&gt; Fold4 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35515]&gt; Fold5 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n\n\nrf_logloss_results &lt;- rf_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") \n\nprint(rf_logloss_results)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n std_err .config        \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1     2 mn_log_loss binary     0.382     5 0.00120 pre0_mod1_post0\n2     4 mn_log_loss binary     0.383     5 0.00128 pre0_mod2_post0\n3     6 mn_log_loss binary     0.387     5 0.00158 pre0_mod3_post0\n4     8 mn_log_loss binary     0.419     5 0.00508 pre0_mod4_post0\n\n\n\n## Select the best forest\nbest_rf &lt;- rf_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nbest_rf\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;dbl&gt; &lt;chr&gt;          \n1     2 pre0_mod1_post0\n\n##Finalize workflow\n\nfinal_rf_wf &lt;- \n  rf_wf %&gt;% \n  finalize_workflow(best_rf)\n\nfinal_rf_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nDiabetes_binary ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  trees = 50\n  min_n = 5\n\nEngine-Specific Arguments:\n  num.threads = parallel::detectCores()\n\nComputational engine: ranger \n\n\nThe best random forest model only used 2 predictors for the mtry hyperparameter.\n\n\n\n\nfinal_rf_fit &lt;- final_rf_wf |&gt;\n  last_fit(split = data_split,\n           metrics = metric_set(mn_log_loss))\n\nfinal_tree_fit &lt;- final_tree_wf |&gt;\n  last_fit(split = data_split,\n           metrics = metric_set(mn_log_loss))\n\n\n# RF\nrf_log_loss &lt;- final_rf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\")\n\n# DTree\ntree_log_loss &lt;- final_tree_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\")\n\n# \nprint(\"RF log loss\")\n\n[1] \"RF log loss\"\n\nrf_log_loss\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.382 pre0_mod0_post0\n\nprint(\"Tree log loss\")\n\n[1] \"Tree log loss\"\n\ntree_log_loss\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.387 pre0_mod0_post0\n\n\nRF has lower log loss, but not by much.\n\n# Random forest predictions\nrf_preds &lt;- final_rf_fit |&gt; collect_predictions()\n\n# Convert probabilities to predicted class\nrf_preds &lt;- rf_preds %&gt;%\n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.5, 1, 0))  # 1 = Yes, 0 = No\n\n# Count number of 0s and 1s\nrf_counts &lt;- rf_preds %&gt;%\n  count(.pred_class)\n\nprint(\"Random Forest predicted counts:\")\n\n[1] \"Random Forest predicted counts:\"\n\nprint(rf_counts)\n\n# A tibble: 1 × 2\n  .pred_class     n\n        &lt;dbl&gt; &lt;int&gt;\n1           0 76104\n\n# Tree predictions\ntree_preds &lt;- final_tree_fit |&gt; collect_predictions()\n\ntree_preds &lt;- tree_preds %&gt;%\n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.5, 1, 0))\n\ntree_counts &lt;- tree_preds %&gt;%\n  count(.pred_class)\n\nprint(\"Tree predicted counts:\")\n\n[1] \"Tree predicted counts:\"\n\nprint(tree_counts)\n\n# A tibble: 2 × 2\n  .pred_class     n\n        &lt;dbl&gt; &lt;int&gt;\n1           0 75985\n2           1   119\n\n\n\nInterestingly, random forest only really predicts 0s on the test set. It is able to minimize log loss by predicting that nobody has diabetes. The decision tree, on teh other hand, does predict that some people have diabetes.\nThis makes sense, since the dataset is so imbalanced.\nAlthough random forest had a slightly lower log loss, I am going to use the decision tree model instead. The values of log_loss were extremely close and the decision tree model doesn’t predict entirely a single class. I will therefore be using it in the API endpoint."
  },
  {
    "objectID": "modeling.html#modeling",
    "href": "modeling.html#modeling",
    "title": "Modeling",
    "section": "",
    "text": "I will be working with the diabetes data set from kaggle.\nMy goal will be to make predictions about if an individual has diabetes/prediabetes based on predictors that are related to that individual’s lifestyle.\nThis document will be modeling the data with a classification tree and random forest.\nI will be ignoring medical predictors, like blood pressure or cholesterol level.\nHere are the final list of predictors I will be using:\n\nIf the person has smoked 100 cigarettes in their lifetime\nIf they had physical activity in the last 30 days\nIf they consume 1 or more fruit per day\nIf they consume 1 or more vegetable in the past day\nIf they are heavy alcohol drinkers\nAge\nEducation\nIncome\n\n\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#load the data\ndata &lt;- read_csv(\"../diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# subset only the relevant predictors\ndata_subset &lt;- data |&gt;\n  select(Diabetes_binary, Smoker, PhysActivity, Fruits, Veggies,\n         HvyAlcoholConsump, Sex, Education, Income)\n\n#convert all features to factors\ndata_subset &lt;- data_subset |&gt;\n  mutate(\n    Diabetes_binary   = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Smoker            = factor(Smoker, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity      = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Fruits            = factor(Fruits, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Veggies           = factor(Veggies, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Sex               = factor(Sex, levels = c(0,1), labels = c(\"Male\", \"Female\")),\n    Education         = as.factor(Education),\n    Income            = as.factor(Income)\n  )\n\n\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nset.seed(222)\n# Put 70% of the data into the training set \ndata_split &lt;- initial_split(data_subset, prop = .7)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\n#check\ndim(test_data)\n\n[1] 76104     9\n\ndim(train_data)\n\n[1] 177576      9\n\n\n\n\n\n\nset.seed(222)\nfolds &lt;- vfold_cv(train_data, v = 5)\n\n\n\n\nThe classification tree is a greedy algorithm that predicts a categorical label, in this case that label is whether someone has diabetes/prediabetes or not.\n\nThe algorithm iteratively splits the data based on the “best” feature. The “best” feature is the feature that maximizes the “purity” of the resulting groups, or that keeps the resulting groups as homogeneous as possible.\nTo split the data, the algorithm divides the data into subsets based on values of the predictor variable.\nThis process is repeated until a desired tree size is reached.\nTo classify a new instance, simply start at the trees root, and follow the nodes down until a leaf is reached.\n\n\nset.seed(222)\n\n#specify the model\n\ntune_spec &lt;- decision_tree(\n  cost_complexity = tune(),  \n  tree_depth = NULL          \n) |&gt;\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_grid &lt;- tibble(\n  cost_complexity = c(0, 0.001, 0.01, 0.05, 0.1)\n)\n\nset.seed(222)\ntree_wf &lt;- workflow() |&gt;\n  add_model(tune_spec) |&gt;\n  add_formula(Diabetes_binary ~ .)\n\ntree_res &lt;- \n  tree_wf |&gt; \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    metrics = metric_set(mn_log_loss)\n    )\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics         .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [142060/35516]&gt; Fold1 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142061/35515]&gt; Fold2 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142061/35515]&gt; Fold3 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35515]&gt; Fold4 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35515]&gt; Fold5 &lt;tibble [5 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n\n\n## View results\n\n\nlogloss_results &lt;- tree_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") \n\nprint(logloss_results)\n\n# A tibble: 5 × 7\n  cost_complexity .metric     .estimator  mean     n std_err .config        \n            &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1           0     mn_log_loss binary     0.388     5 0.00227 pre0_mod1_post0\n2           0.001 mn_log_loss binary     0.404     5 0.00116 pre0_mod2_post0\n3           0.01  mn_log_loss binary     0.404     5 0.00116 pre0_mod3_post0\n4           0.05  mn_log_loss binary     0.404     5 0.00116 pre0_mod4_post0\n5           0.1   mn_log_loss binary     0.404     5 0.00116 pre0_mod5_post0\n\n\nSeems like the tree with 0 as a value of cost complexity did the best. We want to minimize log loss.\n\n## Select the best tree\nbest_tree &lt;- tree_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nbest_tree\n\n# A tibble: 1 × 2\n  cost_complexity .config        \n            &lt;dbl&gt; &lt;chr&gt;          \n1               0 pre0_mod1_post0\n\n##Finalize workflow\n\nfinal_tree_wf &lt;- \n  tree_wf %&gt;% \n  finalize_workflow(best_tree)\n\nfinal_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nDiabetes_binary ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 0\n\nComputational engine: rpart \n\n\n\n\n\nRandom forests are similar to decision trees, except they rely on many trained decision trees. Each individual decision tree is built on a random subset of the predictors. Predictions are made based on voting from the subset of decision trees, which can make it less prone to overfitting and more stable compared to a basic, simple tree.\n\nlibrary(ranger)\n\nset.seed(222)\n\n# Specify the model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),      \n  trees = 50,        \n  min_n = 5    ## make the fitting time faster       \n) |&gt;\n  set_engine(\"ranger\", num.threads = parallel::detectCores()) |&gt;\n  set_mode(\"classification\")\n\n\nrf_grid &lt;- tibble(\n  mtry = c(2, 4, 6, 8) \n)\n\nset.seed(222)\nrf_wf &lt;- workflow() |&gt;\n  add_model(rf_spec) |&gt;\n  add_formula(Diabetes_binary ~ .)\n\nrf_res &lt;-\n  rf_wf |&gt;\n  tune_grid(\n    resamples = folds,\n    grid = rf_grid,\n    metrics = metric_set(mn_log_loss)\n  )\n\nrf_res\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics         .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [142060/35516]&gt; Fold1 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142061/35515]&gt; Fold2 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142061/35515]&gt; Fold3 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35515]&gt; Fold4 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35515]&gt; Fold5 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n\n\nrf_logloss_results &lt;- rf_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"mn_log_loss\") \n\nprint(rf_logloss_results)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n std_err .config        \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1     2 mn_log_loss binary     0.382     5 0.00120 pre0_mod1_post0\n2     4 mn_log_loss binary     0.383     5 0.00128 pre0_mod2_post0\n3     6 mn_log_loss binary     0.387     5 0.00158 pre0_mod3_post0\n4     8 mn_log_loss binary     0.419     5 0.00508 pre0_mod4_post0\n\n\n\n## Select the best forest\nbest_rf &lt;- rf_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\nbest_rf\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;dbl&gt; &lt;chr&gt;          \n1     2 pre0_mod1_post0\n\n##Finalize workflow\n\nfinal_rf_wf &lt;- \n  rf_wf %&gt;% \n  finalize_workflow(best_rf)\n\nfinal_rf_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nDiabetes_binary ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  trees = 50\n  min_n = 5\n\nEngine-Specific Arguments:\n  num.threads = parallel::detectCores()\n\nComputational engine: ranger \n\n\nThe best random forest model only used 2 predictors for the mtry hyperparameter.\n\n\n\n\nfinal_rf_fit &lt;- final_rf_wf |&gt;\n  last_fit(split = data_split,\n           metrics = metric_set(mn_log_loss))\n\nfinal_tree_fit &lt;- final_tree_wf |&gt;\n  last_fit(split = data_split,\n           metrics = metric_set(mn_log_loss))\n\n\n# RF\nrf_log_loss &lt;- final_rf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\")\n\n# DTree\ntree_log_loss &lt;- final_tree_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\")\n\n# \nprint(\"RF log loss\")\n\n[1] \"RF log loss\"\n\nrf_log_loss\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.382 pre0_mod0_post0\n\nprint(\"Tree log loss\")\n\n[1] \"Tree log loss\"\n\ntree_log_loss\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.387 pre0_mod0_post0\n\n\nRF has lower log loss, but not by much.\n\n# Random forest predictions\nrf_preds &lt;- final_rf_fit |&gt; collect_predictions()\n\n# Convert probabilities to predicted class\nrf_preds &lt;- rf_preds %&gt;%\n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.5, 1, 0))  # 1 = Yes, 0 = No\n\n# Count number of 0s and 1s\nrf_counts &lt;- rf_preds %&gt;%\n  count(.pred_class)\n\nprint(\"Random Forest predicted counts:\")\n\n[1] \"Random Forest predicted counts:\"\n\nprint(rf_counts)\n\n# A tibble: 1 × 2\n  .pred_class     n\n        &lt;dbl&gt; &lt;int&gt;\n1           0 76104\n\n# Tree predictions\ntree_preds &lt;- final_tree_fit |&gt; collect_predictions()\n\ntree_preds &lt;- tree_preds %&gt;%\n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.5, 1, 0))\n\ntree_counts &lt;- tree_preds %&gt;%\n  count(.pred_class)\n\nprint(\"Tree predicted counts:\")\n\n[1] \"Tree predicted counts:\"\n\nprint(tree_counts)\n\n# A tibble: 2 × 2\n  .pred_class     n\n        &lt;dbl&gt; &lt;int&gt;\n1           0 75985\n2           1   119\n\n\n\nInterestingly, random forest only really predicts 0s on the test set. It is able to minimize log loss by predicting that nobody has diabetes. The decision tree, on teh other hand, does predict that some people have diabetes.\nThis makes sense, since the dataset is so imbalanced.\nAlthough random forest had a slightly lower log loss, I am going to use the decision tree model instead. The values of log_loss were extremely close and the decision tree model doesn’t predict entirely a single class. I will therefore be using it in the API endpoint."
  }
]